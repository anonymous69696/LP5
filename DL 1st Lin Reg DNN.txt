# Step 1: Import necessary libraries
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Step 2: Load the dataset
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data(
    path='boston_housing_npz',
    test_split=0.2,
    seed=42
)

# Step 3: Data preprocessing
# Convert the dataset into Pandas DataFrames
X_train_df = pd.DataFrame(X_train)
X_test_df = pd.DataFrame(X_test)
y_train_df = pd.DataFrame(y_train)
y_test_df = pd.DataFrame(y_test)

# Normalize the features using Min-Max scaling
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train_df)
X_test_scaled = scaler.transform(X_test_df)

# Split the dataset into training, validation, and test sets
X_train_final, X_val, y_train_final, y_val = train_test_split(X_train_scaled, y_train_df, test_size=0.1, random_state=42)


# Step 4: Build the Neural Network Model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

# Step 5: Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Step 6: Train the model
history = model.fit(X_train_final, y_train_final, epochs=100, batch_size=16, validation_data=(X_val, y_val))

# Step 7: Evaluate the model
loss, mae = model.evaluate(X_test_scaled, y_test_df)
print("Test Loss:", loss)
print("Test MAE:", mae)

# Step 8: Make predictions
predictions = model.predict(X_test_scaled)

# Step 9: Plot the training and validation loss over epochs
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

